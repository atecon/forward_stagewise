set verbose off
clear
set seed 12345
#include fsboost.gfn

set workdir "/home/ninja/repo/fsboost"
USE_INP = FALSE
if USE_INP
    include "./src/fsboost.inp" --force
    include ridge.gfn --force
    include regls.gfn
    include string_utils.gfn
else
    include "./src/fsboost.gfn"
endif

scalar runEX = 4		# select an example

# Select an example
if runEX == 1
    open australia.gdt -q -p    # time-series data set

    # define the variables
    series LHS = ldiff(PAU)
    list RHS = const LHS(-1 to -2) IUS(-1 to -2)  IAU(-1 to -2)

    # Optional: set learning rate a bit higher
    bundle opts = defbundle("learning_rate", 0.01)

elif runEX == 2
    # SIMULATION
    nulldata 750
    set seed 1234
    setobs 1 1 --time-series
    scalar k = 35
    series e = normal()
    series LHS = 1
    series LHS = 4.5 + 0.8*LHS(-1) - 0.4*LHS(-2) + e
    matrix X = mnormal($nobs, k)		# further exogenous
    list RHS = const LHS(-1 to -6)		# arbitrary lags of the endogenous
    loop i=1..cols(X) -q
        RHS += genseries(sprintf("S%d", $i), X[,i])
    endloop

    bundle opts
    #bundle opts = defbundle("learning_rate", 0.01,\
    #                        "early_stopping_strategy", "residual_corr_rel")

elif runEX == 3
    open mroz87.gdt -q -p     # cross-sectional data set

    series LHS = WW
    list RHS = const dataset
    RHS -= LHS WW     # drop lhs variable

    # Set a low learning rate
    bundle opts
    #bundle opts = defbundle("learning_rate", 0.025)

elif runEX == 4             # example taken from Allin's regls pckg.

    # An artificial example with more regressors than observations,
    # where the actually relevant regressors are known. With an s
    # value (lambda/lambda-max) of around 0.3, ADMM is pretty good
    # at picking out the relevant ones. Note that no seed is set, so
    # results will differ somewhat from run to run.

    n = 800  # observations
    k = 120 # regressors
    matrix sel = {5, 20, 37, 60, 70}
    matrix X = mnormal(n, k)
    matrix beta = muniform(n, 1)[sel]
    u = mnormal(n, 1)

    matrix y = sumr(X[,sel] * beta) + u
    printf "Truly non-zero coefficients:\n"
    eval sel
    print beta

    nulldata n --preserve
    series LHS = y
    list RHS = null
    loop i=1..k -q
      RHS += genseries(sprintf("x%d", i), X[,i])
    endloop

    # Set a low learning rate and low early stopping threshold
    bundle opts = defbundle("early_stopping_strategy", "residual_corr_rel",\
                    "learning_rate", 0.015)
endif


# Run standard OLS as benchmark
if nelem(RHS) < $nobs
    #ols LHS RHS
endif


# Run estimation
set stopwatch
bundle B = fsreg(LHS, RHS, opts)
printf "\nTook %g sec.\n", $stopwatch
#print B
print_fsboost_results(B)      # Print estimation results
stop

list X_final = B.X_final    # Retrieve list of selected regressors
eval varnames(X_final)

# Retrieve point estimates
eval B.coeff            # retrieve point estimates of all variables
eval B.coeff_nonzero    # retrieve point estimates of selected variables

series yhat = B.yhat    # Retrieve fitted values

# Plot realizations vs. fitted values
#gnuplot LHS yhat --output="display"

# Plot correlations as a function of iterations
plot_rho_values(B)

# Plot coefficient paths
plot_coefficient_paths(B)

# Prediction
smpl 1 10
matrix preds = fsboost_predict(RHS, B)
print preds



/* Activate if wished
# Define own parameter set
bundle opts = null
scalar opts.verbose = 1
scalar opts.learning_rate = 0.001                  # learning rate
scalar opts.max_num_iterations = 5000   # max. number of boosting rounds
scalar opts.early_stopping_rounds = 20  # stop if no improvement after n rounds

# Run estimation
bundle B = fsreg(LHS, RHS, opts)
print_fsboost_results(B)      # Print estimation results
*/
